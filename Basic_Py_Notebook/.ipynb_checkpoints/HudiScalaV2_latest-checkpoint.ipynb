{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11970d6473794e2ba29373fe8f124777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1610922237578_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-37.ec2.internal:20888/proxy/application_1610922237578_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-73.ec2.internal:8042/node/containerlogs/container_1610922237578_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Int = 4\n"
     ]
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1610922237578_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-37.ec2.internal:20888/proxy/application_1610922237578_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-73.ec2.internal:8042/node/containerlogs/container_1610922237578_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1610922237578_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-37.ec2.internal:20888/proxy/application_1610922237578_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-90-73.ec2.internal:8042/node/containerlogs/container_1610922237578_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c35dce332b54685bba07eeabf84ba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n"
     ]
    }
   ],
   "source": [
    "//Initialize a Spark Session for Hudi\n",
    "\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b91a0ee6145a1900aab18c313db9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputBucket: String = emrpysparkhudi\n",
      "inputDataBucket: String = bdmtest0909\n",
      "hudiTableName: String = indian_food_review_cow\n",
      "hudiTableRecordKey: String = foodid\n",
      "hudiTablePath: String = s3://emrpysparkhudi/createdataset/indian_food_review_cow\n",
      "hudiTablePartitionColumn: String = diet\n",
      "hudiTablePrecombineKey: String = timestamp\n"
     ]
    }
   ],
   "source": [
    "//Where to Store Your Hudi Table \n",
    "val outputBucket = \"emrpysparkhudi\"\n",
    "// input data set \n",
    "val inputDataBucket = \"bdmtest0909\"\n",
    "//Specify common DataSourceWriteOptions int a single hudiOption variable \n",
    "val hudiTableName = \"indian_food_review_cow\"\n",
    "val hudiTableRecordKey = \"foodid\"\n",
    "val hudiTablePath = \"s3://\"+outputBucket+\"/createdataset/\"+hudiTableName\n",
    "val hudiTablePartitionColumn = \"diet\"\n",
    "val hudiTablePrecombineKey = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaf7bef7bb843ed8e4fa802b910d46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read data from s3 \n",
    "val sourceData = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/*\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "sourceData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738ecfc7f5fa4fe0a276564670814cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|foodid|          name|      diet|        state|flavor_profile| course|        state| timestamp|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|     1|    Balu shahi|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610923050|\n",
      "|     2|        Boondi|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610923050|\n",
      "|     3|Gajar ka halwa|vegetarian|       Punjab|         sweet|dessert|       Punjab|1610923050|\n",
      "|     4|        Ghevar|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1610923050|\n",
      "|     5|   Gulab jamun|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610923050|\n",
      "|     6|        Imarti|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610923050|\n",
      "|     7|        Jalebi|vegetarian|Uttar Pradesh|         sweet|dessert|Uttar Pradesh|1610923050|\n",
      "|     8|    Kaju katli|vegetarian|           -1|         sweet|dessert|           -1|1610923050|\n",
      "|     9|      Kalakand|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1610923050|\n",
      "|    10|         Kheer|vegetarian|           -1|         sweet|dessert|           -1|1610923050|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sourceData.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd2fb1ce05c4b4283ad1745d8acd8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> diet, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> indian_food_review_cow, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> foodid, hoodie.table.name -> indian_food_review_cow, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.write.partitionpath.field -> diet)\n"
     ]
    }
   ],
   "source": [
    "// Set up our Hudi Data Source Options\n",
    "val hudiOptions = Map[String,String](\n",
    "    HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "    //for this detaset use COPY_ON_WRITE storage strategy other option us MERGE_ON_READ\n",
    "    DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "    //next three options configure what Hudi should use as its record key \n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"foodid\",\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "    //for this data set, we specify that we want to sync metadata with Hive \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "    DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    "    )\n",
    "\n",
    "//getName,\n",
    "//    \"hoodie.parquet.max.file.size\" -> String.valueOf(1024 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.small.file.limit\" -> String.valueOf(64 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.compression.ratio\" -> String.valueOf(0.5),\n",
    "//    \"hoodie.insert.shuffle.parallelism\" -> String.valueOf(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a145930e94487b9161fa0aecab09d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Write input data to Hudi \n",
    "\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    " // Opreation  Key tells Hudi whether this is an Insert Upsert or Bulk Insert operation \n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee777c796a34d82930e7c0ea843ee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "// read Hudi data set from s3 \n",
    "\n",
    "val readOptimzedHudiViewDF = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f38357e320425ab51ed8821c24c24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// take a look at our data ... \n",
    "readOptimzedHudiViewDF.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad81a4b2b4264b2ab6d964eb1e69966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "// select row and update - null \n",
    "\n",
    "val upsertdf = (readOptimzedHudiViewDF.filter($\"state\" === -1).withColumn(\"state\",lit(\"India\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69837d4b0e584d62bf2bf0042c8c9be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|India|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(upsertdf.select(upsertdf(\"state\")).distinct).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc07ade6c04e1da084cf1bb2e6028c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// IMP - Before , if you wanted to update data in s3 ,, you have to read the old data , merge with the new data\n",
    "// and then overwrite the old data .. with Hudi , we can directly update the data in-Place\n",
    "\n",
    "(upsertdf.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .options(hudiOptions)\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef08f53912a7448889aed042f51c8377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_updated: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_updated = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_updated.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdd1f95ce354aeb9e6f1e755d260baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleteRowDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|foodid|    name|         ingredients|          diet|prep_time|cook_time|flavor_profile|     course|state|region| timestamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|     20210117224444|20210117224444_0_507|               252|            vegetarian|e8e8a91e-2ada-4d2...|   252| Bebinca|Coconut milk, egg...|    vegetarian|       20|       60|         sweet|    dessert|  Goa|  West|1610923050|\n",
      "|     20210117224444|20210117224444_0_510|               255|            vegetarian|e8e8a91e-2ada-4d2...|   255|  Pinaca|Brown rice, fenne...|    vegetarian|       -1|       -1|         sweet|    dessert|  Goa|  West|1610923050|\n",
      "|     20210117224444|20210117224444_1_347|               212|        non vegetarian|ceaa5243-86b8-43b...|   212|Vindaloo|Chicken, coconut ...|non vegetarian|       10|       40|         spicy|main course|  Goa|  West|1610923050|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// delete operation \n",
    "\n",
    "val deleteRowDF = readOptimzedHudiViewDF_updated.filter($\"state\" === \"Goa\");\n",
    "deleteRowDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f0fb64e7041d2af8463e579630bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(deleteRowDF.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .options(hudiOptions)\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\")\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath))\n",
    "\n",
    "\n",
    "//You can also hard delete data by setting OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL to\n",
    "//remove all records in the dataset you submit. For instructions on performing soft deletes, \n",
    "//and for more information about deleting data stored in Hudi tables, see Deletes in the Apache Hudi documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01338f4e251d4ab39ddc4fe6fe2d536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_postdelete: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|          India|      24|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_postdelete = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_postdelete.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172d7d77a8614a7fbbec70cd1e1cffe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20210117224444, 20210117224822)\n",
      "res71: String = [Ljava.lang.String;@1d007c17\n"
     ]
    }
   ],
   "source": [
    "// Point in time SQL \n",
    "\n",
    "val commits = (spark.sql(\"\"\" select distinct(_hoodie_commit_time) as commitTime from \n",
    "indian_food_ro_table order by commitTime\"\"\").map(k=> k.getString(0)).take(50))\n",
    "\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87e07dcb3a041b887bee0b05db9d687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res72: String = 20210117224444\n"
     ]
    }
   ],
   "source": [
    "commits(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e64ae0a1e84cada215b57e803753c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.hudi.DataSourceReadOptions\n",
      "beginTime: String = 0\n",
      "endTime: String = 20210117224444\n",
      "dataSet: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.hudi.DataSourceReadOptions\n",
    "val beginTime = \"0\"\n",
    "\n",
    "val endTime = commits(0)\n",
    "\n",
    "val dataSet = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b42e4f02e448049df632da1a16f59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataSet.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d445288ec4f6415aa93f998eaa73042c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 20210117224444\n",
      "endTime: String = 20210117224822\n",
      "dataSet1: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "val beginTime = commits(0)\n",
    "\n",
    "val endTime = commits(1)\n",
    "\n",
    "val dataSet1 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2c19ba4d454031a31a10152d094f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+-----+--------+\n",
      "|state|count(1)|\n",
      "+-----+--------+\n",
      "|India|      24|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataSet1.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857245b8df1d47d1a47ac62217531da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 20210117224822\n",
      "dataSet2: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n"
     ]
    }
   ],
   "source": [
    "val beginTime = commits(1)\n",
    "\n",
    "//val endTime = commits(1)\n",
    "\n",
    "val dataSet2 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     //.option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb088909cc6d4ee7b0a56be051473dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+-----+--------+\n",
      "|state|count(1)|\n",
      "+-----+--------+\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataSet2.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
